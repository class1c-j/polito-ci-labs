{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "- Submission: Sunday, December 17 ([CET](https://www.timeanddate.com/time/zones/cet))\n",
    "- Reviews: Dies Natalis Solis Invicti ([CET](https://en.wikipedia.org/wiki/Sol_Invictus))\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Reviews will be assigned on Monday, December 4\n",
    "- You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import choice\n",
    "from copy import deepcopy\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC_SQUARE = [2, 7, 6, 9, 5, 1, 4, 3, 8]\n",
    "GOAL_SUM = 15\n",
    "BOARD_SIZE = 3\n",
    "\n",
    "X = 0\n",
    "O = 1\n",
    "\n",
    "\n",
    "class BoardState:\n",
    "    def __init__(self, x=None, o=None) -> None:\n",
    "        self.x_plays = x if x is not None else set()\n",
    "        self.o_plays = o if o is not None else set()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        board_string = \"\"\n",
    "        for i in range(BOARD_SIZE):\n",
    "            for j in range(BOARD_SIZE):\n",
    "                index = i * BOARD_SIZE + j\n",
    "                if MAGIC_SQUARE[index] in self.x_plays:\n",
    "                    board_string += \"| X \"\n",
    "                elif MAGIC_SQUARE[index] in self.o_plays:\n",
    "                    board_string += \"| O \"\n",
    "                else:\n",
    "                    board_string += \"|   \"\n",
    "            board_string += \"|\\n\"\n",
    "        return board_string\n",
    "\n",
    "    def x_win(self) -> bool:\n",
    "        return any(sum(c) == GOAL_SUM for c in combinations(self.x_plays, BOARD_SIZE))\n",
    "\n",
    "    def o_win(self) -> bool:\n",
    "        return any(sum(c) == GOAL_SUM for c in combinations(self.o_plays, BOARD_SIZE))\n",
    "\n",
    "    def is_over(self) -> bool:\n",
    "        return not self.get_available() or self.x_win() or self.o_win()\n",
    "\n",
    "    def value(self) -> int:\n",
    "        if self.x_win():\n",
    "            return 1\n",
    "        elif self.o_win:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def play(self, square: int, player: int) -> None:\n",
    "        if player == 0:\n",
    "            self.x_plays.add(square)\n",
    "        else:\n",
    "            self.o_plays.add(square)\n",
    "\n",
    "    def get_available(self) -> set:\n",
    "        return set(MAGIC_SQUARE) - self.x_plays - self.o_plays\n",
    "\n",
    "    def get_hashable(self) -> tuple[frozenset]:\n",
    "        return frozenset(self.x_plays), frozenset(self.o_plays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_game():\n",
    "    history = list()\n",
    "    state = BoardState()\n",
    "    available = list(state.get_available())\n",
    "    while available:\n",
    "        x = choice(available)\n",
    "        state.play(x, X)\n",
    "        available.remove(x)\n",
    "        history.append(deepcopy(state))\n",
    "        if state.is_over():\n",
    "            break\n",
    "\n",
    "        o = choice(available)\n",
    "        state.play(o, O)\n",
    "        available.remove(o)\n",
    "        history.append(deepcopy(state))\n",
    "        if state.is_over():\n",
    "            break\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| X |   | O |\n",
      "|   | X |   |\n",
      "| O |   | X |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "last = random_game()[-1]\n",
    "print(last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montecarlo reinforcement learning\n",
    "\n",
    "Code adapted from the lecture.\n",
    "Here, every episode, a game is played randomly, and the value of each state the game was in is updated according to $V(s_t) \\leftarrow V(s_t) + \\alpha [G_t - V(s_t)]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac93a3d03c394fa9bb2bda9032245126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "value_dict = defaultdict(float)\n",
    "hit_state = defaultdict(int)\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "EPISODES = 1_000_000\n",
    "\n",
    "for steps in tqdm(range(EPISODES)):\n",
    "    history = random_game()\n",
    "    final_reward = history[-1].value()\n",
    "    for state in history:\n",
    "        hashable_state = state.get_hashable()\n",
    "        hit_state[hashable_state] += 1\n",
    "        value_dict[hashable_state] = value_dict[hashable_state] + LEARNING_RATE * (\n",
    "            final_reward - value_dict[hashable_state]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((frozenset({1, 6, 7, 8, 9}), frozenset({2, 3, 4, 5})), 0.38916270402752223),\n",
       " ((frozenset({2, 3, 6, 7, 9}), frozenset({1, 4, 5, 8})), 0.3881845331165637),\n",
       " ((frozenset({3, 4, 7, 8, 9}), frozenset({1, 2, 5, 6})), 0.38806215166627545),\n",
       " ((frozenset({2, 5, 7, 8, 9}), frozenset({1, 3, 4, 6})), 0.38806215166627545),\n",
       " ((frozenset({1, 3, 4, 7, 8}), frozenset({2, 5, 6, 9})), 0.3859779175784625),\n",
       " ((frozenset({4, 5, 6, 7, 9}), frozenset({1, 2, 3, 8})), 0.3850561472106083),\n",
       " ((frozenset({1, 2, 3, 5, 8}), frozenset({4, 6, 7, 9})), 0.38444086500336827),\n",
       " ((frozenset({5}), frozenset()), 0.3843652951937697),\n",
       " ((frozenset({1, 3, 6, 8, 9}), frozenset({2, 4, 5, 7})), 0.3841329930804508),\n",
       " ((frozenset({1, 2, 4, 5, 8}), frozenset({3, 6, 7, 9})), 0.3837633435097312)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top = sorted(value_dict.items(), key=lambda e: e[1], reverse=True)[:10]\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| O | X | X |\n",
      "| X | O | X |\n",
      "| O | O | X |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(BoardState(set(top[0][0][0]), set(top[0][0][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_action(state: BoardState, action: int, player: int) -> BoardState:\n",
    "    new_state = deepcopy(state)\n",
    "    new_state.play(action, player)\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52c0888e1dc477d9722d5e9a21b6428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q_table = defaultdict(lambda: 0.0, {})\n",
    "\n",
    "\n",
    "DISCOUNT_RATE = 0.9\n",
    "LEARNING_RATE = 1e-4\n",
    "EPISODES = 1_000_000\n",
    "start_state = BoardState()\n",
    "\n",
    "for episode in tqdm(range(EPISODES)):\n",
    "    current_state = start_state\n",
    "    total_reward = 0\n",
    "\n",
    "    while not (current_state.x_win() or current_state.o_win()) and len(\n",
    "        current_state.get_available()\n",
    "    ):\n",
    "        new_action = choice(list(current_state.get_available()))\n",
    "        new_state = do_action(current_state, new_action, X)\n",
    "        new_reward = new_state.value()\n",
    "        hashable_state = new_state.get_hashable()\n",
    "\n",
    "        current_q = q_table[(hashable_state, new_action)]\n",
    "\n",
    "        possible_states = [\n",
    "            ((hashable_state, a), do_action(new_state, a, O).get_hashable())\n",
    "            for a in list(new_state.get_available())\n",
    "        ]\n",
    "\n",
    "        next_actions_and_states = [\n",
    "            q_table[state_action] for _, state_action in possible_states\n",
    "        ]\n",
    "\n",
    "        if next_actions_and_states:\n",
    "            max_next_q = max(next_actions_and_states)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        target_q = new_reward + DISCOUNT_RATE * max_next_q\n",
    "        q_table[(hashable_state, new_action)] = q_table.get(\n",
    "            (hashable_state, new_action), 0\n",
    "        ) + LEARNING_RATE * (target_q - current_q)\n",
    "\n",
    "        total_reward += new_reward\n",
    "\n",
    "        new_state = do_action(new_state, choice(list(new_state.get_available())), O)\n",
    "        current_state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_move(state: BoardState) -> int:\n",
    "    possible = list(state.get_available())\n",
    "    best_q = float(\"-inf\")\n",
    "    best_move = None\n",
    "\n",
    "    for move in possible:\n",
    "        move_q = q_table[(state.get_hashable(), move)]\n",
    "        if move_q > best_q:\n",
    "            best_q = move_q\n",
    "            best_move = move\n",
    "\n",
    "    return best_move\n",
    "\n",
    "\n",
    "def random_move(state: BoardState) -> int:\n",
    "    possible = list(state.get_available())\n",
    "    return choice(possible)\n",
    "\n",
    "\n",
    "def game(selector_x: callable, selector_o: callable) -> BoardState:\n",
    "    current_state = BoardState()\n",
    "    current_player = X\n",
    "\n",
    "    while not (current_state.is_over()):\n",
    "        if current_player == X:\n",
    "            action = selector_x(current_state)\n",
    "        else:\n",
    "            action = selector_o(current_state)\n",
    "\n",
    "        temp_state = deepcopy(current_state)\n",
    "        temp_state.play(action, current_player)\n",
    "        current_state = temp_state\n",
    "\n",
    "        current_player = 1 - current_player\n",
    "\n",
    "    return current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-LEARNING AGAINST RANDOM Wins: 2514, Draws: 965, Losses: 1521\n",
      "Winning Percentage: 50.28%\n",
      "Draw Percentage: 19.30%\n",
      "Loss Percentage: 30.42%\n"
     ]
    }
   ],
   "source": [
    "SIMULATIONS = 5_000\n",
    "\n",
    "\n",
    "def simulate_games(n_games: int, move_x: callable, move_y: callable) -> tuple[int]:\n",
    "    x_wins = 0\n",
    "    o_wins = 0\n",
    "    draws = 0\n",
    "    for _ in range(n_games):\n",
    "        game_state = game(move_x, move_y)\n",
    "        if game_state.x_win():\n",
    "            x_wins += 1\n",
    "        elif game_state.o_win():\n",
    "            o_wins += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "    return x_wins, draws, o_wins\n",
    "\n",
    "\n",
    "q_wins, draws, opponent_wins = simulate_games(SIMULATIONS, q_move, random_move)\n",
    "\n",
    "total_games = q_wins + opponent_wins + draws\n",
    "winning_percentage = q_wins / total_games if total_games != 0 else 0\n",
    "draw_percentage = draws / total_games if total_games != 0 else 0\n",
    "loss_percentage = opponent_wins / total_games if total_games != 0 else 0\n",
    "\n",
    "print(\n",
    "    f\"Q-LEARNING AGAINST RANDOM Wins: {q_wins}, Draws: {draws}, Losses: {opponent_wins}\"\n",
    ")\n",
    "print(\n",
    "    f\"Winning Percentage: {winning_percentage:.2%}\\nDraw Percentage: {draw_percentage:.2%}\\nLoss Percentage: {loss_percentage:.2%}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
