{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "- Submission: Sunday, December 17 ([CET](https://www.timeanddate.com/time/zones/cet))\n",
    "- Reviews: Dies Natalis Solis Invicti ([CET](https://en.wikipedia.org/wiki/Sol_Invictus))\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Reviews will be assigned on Monday, December 4\n",
    "- You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import choice, randint, random\n",
    "from copy import deepcopy\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC_SQUARE = [2, 7, 6, 9, 5, 1, 4, 3, 8]\n",
    "GOAL_SUM = 15\n",
    "BOARD_SIZE = 3\n",
    "\n",
    "X = 0\n",
    "O = 1\n",
    "\n",
    "\n",
    "class BoardState:\n",
    "    def __init__(self, x=None, o=None) -> None:\n",
    "        self.x_plays = x if x is not None else set()\n",
    "        self.o_plays = o if o is not None else set()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        board_string = \"\"\n",
    "        for i in range(BOARD_SIZE):\n",
    "            for j in range(BOARD_SIZE):\n",
    "                index = i * BOARD_SIZE + j\n",
    "                if MAGIC_SQUARE[index] in self.x_plays:\n",
    "                    board_string += \"| X \"\n",
    "                elif MAGIC_SQUARE[index] in self.o_plays:\n",
    "                    board_string += \"| O \"\n",
    "                else:\n",
    "                    board_string += \"|   \"\n",
    "            board_string += \"|\\n\"\n",
    "        return board_string\n",
    "\n",
    "    def x_win(self) -> bool:\n",
    "        return any(sum(c) == GOAL_SUM for c in combinations(self.x_plays, BOARD_SIZE))\n",
    "\n",
    "    def o_win(self) -> bool:\n",
    "        return any(sum(c) == GOAL_SUM for c in combinations(self.o_plays, BOARD_SIZE))\n",
    "\n",
    "    def is_over(self) -> bool:\n",
    "        return not self.get_available() or self.x_win() or self.o_win()\n",
    "\n",
    "    def value(self) -> int:\n",
    "        if self.x_win():\n",
    "            return 1\n",
    "        elif self.o_win():\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def play(self, square: int, player: int) -> None:\n",
    "        if player == 0:\n",
    "            self.x_plays.add(square)\n",
    "        else:\n",
    "            self.o_plays.add(square)\n",
    "\n",
    "    def get_available(self) -> set:\n",
    "        return set(MAGIC_SQUARE) - self.x_plays - self.o_plays\n",
    "\n",
    "    def get_hashable(self) -> tuple[frozenset]:\n",
    "        return frozenset(self.x_plays), frozenset(self.o_plays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_game():\n",
    "    history = list()\n",
    "    state = BoardState()\n",
    "    available = list(state.get_available())\n",
    "    while available:\n",
    "        x = choice(available)\n",
    "        state.play(x, X)\n",
    "        available.remove(x)\n",
    "        history.append(deepcopy(state))\n",
    "        if state.is_over():\n",
    "            break\n",
    "\n",
    "        o = choice(available)\n",
    "        state.play(o, O)\n",
    "        available.remove(o)\n",
    "        history.append(deepcopy(state))\n",
    "        if state.is_over():\n",
    "            break\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| X | O | O |\n",
      "| X | O | X |\n",
      "| X |   |   |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "last = random_game()[-1]\n",
    "print(last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montecarlo reinforcement learning\n",
    "\n",
    "Code adapted from the lecture.\n",
    "Here, every episode, a game is played randomly, and the value of each state the game was in is updated according to $V(s_t) \\leftarrow V(s_t) + \\alpha [G_t - V(s_t)]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f0baddaaae4cd0a8d8f067e3b7bda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "value_dict = defaultdict(float)\n",
    "hit_state = defaultdict(int)\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "EPISODES = 1_000_000\n",
    "\n",
    "for steps in tqdm(range(EPISODES)):\n",
    "    history = random_game()\n",
    "    final_reward = history[-1].value()\n",
    "    for state in history:\n",
    "        hashable_state = state.get_hashable()\n",
    "        hit_state[hashable_state] += 1\n",
    "        value_dict[hashable_state] = value_dict[hashable_state] + LEARNING_RATE * (\n",
    "            final_reward - value_dict[hashable_state]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "Here I implemented a Q-Learning agent, applying the formula $Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha(R_{t+1} + \\gamma max_a Q(S_{t+1}, a) - Q(S_t, A_t))$ to update the q-table. The q-table is updated every action (instead of every episode like in Montecarlo), which means that we can take advantage of that to reward specific behaviours such as blocking an opponent's win.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_action(state: BoardState, action: int, player: int) -> BoardState:\n",
    "    new_state = deepcopy(state)\n",
    "    new_state.play(action, player)\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(current_state: BoardState, next_state: BoardState, action: int) -> int:\n",
    "    reward = 0\n",
    "    if is_block(current_state, action):\n",
    "        reward += 5\n",
    "    elif didnt_block(current_state, action):\n",
    "        reward += -5\n",
    "    if next_state.x_win():\n",
    "        reward += 10\n",
    "    elif next_state.o_win():\n",
    "        reward += -10\n",
    "    return reward\n",
    "\n",
    "\n",
    "def is_block(state: BoardState, action: int) -> bool:\n",
    "    opponent_tiles = state.o_plays\n",
    "    return sum(opponent_tiles) + action == GOAL_SUM\n",
    "\n",
    "\n",
    "def didnt_block(state: BoardState, action: int) -> bool:\n",
    "    opponent_tiles = state.o_plays\n",
    "    available_actions = state.get_available()\n",
    "    block = any(sum(opponent_tiles) + a == GOAL_SUM for a in available_actions)\n",
    "    return block and (sum(opponent_tiles) + action != GOAL_SUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd93d33ad9040f580dd8ca5824af4de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q_table = defaultdict(lambda: 0.0)\n",
    "\n",
    "\n",
    "DISCOUNT_RATE = 0.99\n",
    "LEARNING_RATE = 1e-4\n",
    "EPISODES = 500_000\n",
    "\n",
    "start_state = BoardState()\n",
    "\n",
    "for episode in tqdm(range(EPISODES)):\n",
    "    current_state = start_state\n",
    "    total_reward = 0\n",
    "\n",
    "    while not current_state.is_over():\n",
    "        # x turn -> St\n",
    "        new_action = choice(list(current_state.get_available()))\n",
    "\n",
    "        # x plays, o turn -> St+1\n",
    "        new_state = do_action(current_state, new_action, X)\n",
    "        new_reward = reward(current_state, new_state, new_action)\n",
    "        hashable_state_action = (current_state.get_hashable(), new_action)\n",
    "\n",
    "        current_q = q_table[hashable_state_action]\n",
    "\n",
    "        if new_state.get_available():\n",
    "            # o plays x turn -> St+2\n",
    "            state_t_plus_two = do_action(\n",
    "                new_state, choice(list(new_state.get_available())), O\n",
    "            )\n",
    "        else:\n",
    "            # terminal state\n",
    "            state_t_plus_two = new_state\n",
    "\n",
    "        possible_states_actions = [\n",
    "            (do_action(state_t_plus_two, a, X).get_hashable(), a)\n",
    "            for a in new_state.get_available()\n",
    "        ]\n",
    "\n",
    "        next_actions_and_states = [\n",
    "            q_table[state_action] for state_action in possible_states_actions\n",
    "        ]\n",
    "\n",
    "        if next_actions_and_states and state_t_plus_two != new_state:\n",
    "            max_next_q = np.max(next_actions_and_states)\n",
    "        else:\n",
    "            max_next_q = 0\n",
    "\n",
    "        target_q = new_reward + DISCOUNT_RATE * max_next_q\n",
    "        q_table[hashable_state_action] += LEARNING_RATE * (target_q - current_q)\n",
    "\n",
    "        total_reward += new_reward\n",
    "\n",
    "        current_state = state_t_plus_two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification on q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d2da5fec724b6fbd61e474441431c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q_table_v2 = defaultdict(lambda: 0.0)\n",
    "\n",
    "\n",
    "DISCOUNT_RATE = 0.99\n",
    "LEARNING_RATE = 0.01\n",
    "EPISODES = 500_000\n",
    "start_state = BoardState()\n",
    "\n",
    "\n",
    "for episode in tqdm(range(EPISODES)):\n",
    "    current_state = start_state\n",
    "    total_reward = 0\n",
    "\n",
    "    while not current_state.is_over():\n",
    "        new_action = choice(list(current_state.get_available()))\n",
    "\n",
    "        new_state = do_action(current_state, new_action, X)\n",
    "        new_reward = reward(current_state, new_state, new_action)\n",
    "        hashable_state_action = (current_state.get_hashable(), new_action)\n",
    "\n",
    "        current_q = q_table_v2[hashable_state_action]\n",
    "\n",
    "        possible_states_actions = [\n",
    "            (do_action(new_state, a, O).get_hashable(), a)\n",
    "            for a in new_state.get_available()\n",
    "        ]\n",
    "\n",
    "        next_actions_and_states = [\n",
    "            q_table_v2[state_action] for state_action in possible_states_actions\n",
    "        ]\n",
    "\n",
    "        if next_actions_and_states:\n",
    "            max_next_q = -np.min(next_actions_and_states)\n",
    "        else:\n",
    "            max_next_q = 0\n",
    "\n",
    "        target_q = new_reward + DISCOUNT_RATE * max_next_q\n",
    "        q_table_v2[hashable_state_action] += LEARNING_RATE * (target_q - current_q)\n",
    "\n",
    "        total_reward += new_reward\n",
    "\n",
    "        if new_state.get_available():\n",
    "            new_state = do_action(new_state, choice(list(new_state.get_available())), O)\n",
    "        current_state = new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_move(state: BoardState) -> int:\n",
    "    possible = state.get_available()\n",
    "    best_q = float(\"-inf\")\n",
    "    best_move = None\n",
    "\n",
    "    for move in possible:\n",
    "        move_q = q_table[(state.get_hashable(), move)]\n",
    "        if move_q > best_q:\n",
    "            best_q = move_q\n",
    "            best_move = move\n",
    "\n",
    "    return best_move\n",
    "\n",
    "\n",
    "def my_q_move(state: BoardState) -> int:\n",
    "    possible = state.get_available()\n",
    "    best_q = float(\"-inf\")\n",
    "    best_move = None\n",
    "\n",
    "    for move in possible:\n",
    "        move_q = q_table_v2[(state.get_hashable(), move)]\n",
    "        if move_q > best_q:\n",
    "            best_q = move_q\n",
    "            best_move = move\n",
    "\n",
    "    return best_move\n",
    "\n",
    "\n",
    "def mc_move(state: BoardState) -> int:\n",
    "    possible = state.get_available()\n",
    "    best_q = float(\"-inf\")\n",
    "    best_move = None\n",
    "\n",
    "    for move in possible:\n",
    "        move_q = value_dict[(state.get_hashable(), move)]\n",
    "        if move_q > best_q:\n",
    "            best_q = move_q\n",
    "            best_move = move\n",
    "    return best_move\n",
    "\n",
    "\n",
    "def random_move(state: BoardState) -> int:\n",
    "    possible = list(state.get_available())\n",
    "    return choice(possible)\n",
    "\n",
    "\n",
    "def game(selector_x: callable, selector_o: callable) -> BoardState:\n",
    "    current_state = BoardState()\n",
    "    current_player = X\n",
    "\n",
    "    while not (current_state.is_over()):\n",
    "        if current_player == X:\n",
    "            action = selector_x(current_state)\n",
    "        else:\n",
    "            action = selector_o(current_state)\n",
    "\n",
    "        temp_state = deepcopy(current_state)\n",
    "        temp_state.play(action, current_player)\n",
    "        current_state = temp_state\n",
    "\n",
    "        current_player = 1 - current_player\n",
    "\n",
    "    return current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-LEARNING AGAINST RANDOM Wins: 4172, Draws: 355, Losses: 473\n",
      "Winning Percentage: 83.44%\n",
      "Draw Percentage: 7.10%\n",
      "Loss Percentage: 9.46%\n",
      "MY Q-LEARNING AGAINST RANDOM Wins: 4253, Draws: 344, Losses: 403\n",
      "Winning Percentage: 85.06%\n",
      "Draw Percentage: 6.88%\n",
      "Loss Percentage: 8.06%\n",
      "MONTECARLO AGAINST RANDOM Wins: 2508, Draws: 998, Losses: 1494\n",
      "Winning Percentage: 50.16%\n",
      "Draw Percentage: 19.96%\n",
      "Loss Percentage: 29.88%\n"
     ]
    }
   ],
   "source": [
    "SIMULATIONS = 5_000\n",
    "\n",
    "\n",
    "def simulate_games(n_games: int, move_x: callable, move_y: callable) -> tuple[int]:\n",
    "    x_wins = 0\n",
    "    o_wins = 0\n",
    "    draws = 0\n",
    "    for _ in range(n_games):\n",
    "        game_state = game(move_x, move_y)\n",
    "        if game_state.x_win():\n",
    "            x_wins += 1\n",
    "        elif game_state.o_win():\n",
    "            o_wins += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "    return x_wins, draws, o_wins\n",
    "\n",
    "\n",
    "def print_results(player_name, q_wins, draws, opponent_wins):\n",
    "    total_games = q_wins + opponent_wins + draws\n",
    "    winning_percentage = q_wins / total_games if total_games != 0 else 0\n",
    "    draw_percentage = draws / total_games if total_games != 0 else 0\n",
    "    loss_percentage = opponent_wins / total_games if total_games != 0 else 0\n",
    "\n",
    "    print(f\"{player_name} Wins: {q_wins}, Draws: {draws}, Losses: {opponent_wins}\")\n",
    "    print(\n",
    "        f\"Winning Percentage: {winning_percentage:.2%}\\nDraw Percentage: {draw_percentage:.2%}\\nLoss Percentage: {loss_percentage:.2%}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def simulate_and_print(player_name, move_x):\n",
    "    q_wins, draws, opponent_wins = simulate_games(SIMULATIONS, move_x, random_move)\n",
    "    print_results(player_name, q_wins, draws, opponent_wins)\n",
    "\n",
    "\n",
    "simulate_and_print(\"Q-LEARNING AGAINST RANDOM\", q_move)\n",
    "\n",
    "\n",
    "simulate_and_print(\"MY Q-LEARNING AGAINST RANDOM\", my_q_move)\n",
    "\n",
    "\n",
    "simulate_and_print(\"MONTECARLO AGAINST RANDOM\", mc_move)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
