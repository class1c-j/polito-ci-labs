{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "- Submission: Sunday, December 17 ([CET](https://www.timeanddate.com/time/zones/cet))\n",
    "- Reviews: Dies Natalis Solis Invicti ([CET](https://en.wikipedia.org/wiki/Sol_Invictus))\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Reviews will be assigned on Monday, December 4\n",
    "- You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import choice, randint, random\n",
    "from copy import deepcopy\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC_SQUARE = [2, 7, 6, 9, 5, 1, 4, 3, 8]\n",
    "GOAL_SUM = 15\n",
    "BOARD_SIZE = 3\n",
    "\n",
    "X = 0\n",
    "O = 1\n",
    "\n",
    "\n",
    "class BoardState:\n",
    "    def __init__(self, x=None, o=None) -> None:\n",
    "        self.x_plays = x if x is not None else set()\n",
    "        self.o_plays = o if o is not None else set()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        board_string = \"\"\n",
    "        for i in range(BOARD_SIZE):\n",
    "            for j in range(BOARD_SIZE):\n",
    "                index = i * BOARD_SIZE + j\n",
    "                if MAGIC_SQUARE[index] in self.x_plays:\n",
    "                    board_string += \"| X \"\n",
    "                elif MAGIC_SQUARE[index] in self.o_plays:\n",
    "                    board_string += \"| O \"\n",
    "                else:\n",
    "                    board_string += \"|   \"\n",
    "            board_string += \"|\\n\"\n",
    "        return board_string\n",
    "\n",
    "    def x_win(self) -> bool:\n",
    "        return any(sum(c) == GOAL_SUM for c in combinations(self.x_plays, BOARD_SIZE))\n",
    "\n",
    "    def o_win(self) -> bool:\n",
    "        return any(sum(c) == GOAL_SUM for c in combinations(self.o_plays, BOARD_SIZE))\n",
    "\n",
    "    def is_over(self) -> bool:\n",
    "        return not self.get_available() or self.x_win() or self.o_win()\n",
    "\n",
    "    def value(self) -> int:\n",
    "        if self.x_win():\n",
    "            return 1\n",
    "        elif self.o_win:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def play(self, square: int, player: int) -> None:\n",
    "        if player == 0:\n",
    "            self.x_plays.add(square)\n",
    "        else:\n",
    "            self.o_plays.add(square)\n",
    "\n",
    "    def get_available(self) -> set:\n",
    "        return set(MAGIC_SQUARE) - self.x_plays - self.o_plays\n",
    "\n",
    "    def get_hashable(self) -> tuple[frozenset]:\n",
    "        return frozenset(self.x_plays), frozenset(self.o_plays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_game():\n",
    "    history = list()\n",
    "    state = BoardState()\n",
    "    available = list(state.get_available())\n",
    "    while available:\n",
    "        x = choice(available)\n",
    "        state.play(x, X)\n",
    "        available.remove(x)\n",
    "        history.append(deepcopy(state))\n",
    "        if state.is_over():\n",
    "            break\n",
    "\n",
    "        o = choice(available)\n",
    "        state.play(o, O)\n",
    "        available.remove(o)\n",
    "        history.append(deepcopy(state))\n",
    "        if state.is_over():\n",
    "            break\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| O | O | X |\n",
      "| X | X | X |\n",
      "| O |   |   |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "last = random_game()[-1]\n",
    "print(last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montecarlo reinforcement learning\n",
    "\n",
    "Code adapted from the lecture.\n",
    "Here, every episode, a game is played randomly, and the value of each state the game was in is updated according to $V(s_t) \\leftarrow V(s_t) + \\alpha [G_t - V(s_t)]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b25615c65b465f8af4a3adafec7115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "value_dict = defaultdict(float)\n",
    "hit_state = defaultdict(int)\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "EPISODES = 1_000_000\n",
    "\n",
    "for steps in tqdm(range(EPISODES)):\n",
    "    history = random_game()\n",
    "    final_reward = history[-1].value()\n",
    "    for state in history:\n",
    "        hashable_state = state.get_hashable()\n",
    "        hit_state[hashable_state] += 1\n",
    "        value_dict[hashable_state] = value_dict[hashable_state] + LEARNING_RATE * (\n",
    "            final_reward - value_dict[hashable_state]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((frozenset({1, 3, 4, 7, 8}), frozenset({2, 5, 6, 9})), 0.38995631623385146),\n",
       " ((frozenset({5}), frozenset()), 0.3875577017337914),\n",
       " ((frozenset({1, 3, 4, 5, 7}), frozenset({2, 6, 8, 9})), 0.3862848672236049),\n",
       " ((frozenset({1, 2, 3, 4, 8}), frozenset({5, 6, 7, 9})), 0.38603931978670464),\n",
       " ((frozenset({1, 4, 5, 7, 9}), frozenset({2, 3, 6, 8})), 0.38474858302110837),\n",
       " ((frozenset({1, 2, 4, 6, 7}), frozenset({3, 5, 8, 9})), 0.3843177347071323),\n",
       " ((frozenset({2, 4, 7, 8, 9}), frozenset({1, 3, 5, 6})), 0.38394819602019487),\n",
       " ((frozenset({2, 6, 7, 8, 9}), frozenset({1, 3, 4, 5})), 0.38320845311068497),\n",
       " ((frozenset({4, 5, 6, 7, 8}), frozenset({1, 2, 3, 9})), 0.3828382486217784),\n",
       " ((frozenset({1, 2, 3, 5, 7}), frozenset({4, 6, 8, 9})), 0.3825913221927066)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top = sorted(value_dict.items(), key=lambda e: e[1], reverse=True)[:10]\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| O | X | O |\n",
      "| O | O | X |\n",
      "| X | X | X |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(BoardState(set(top[0][0][0]), set(top[0][0][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "Here I implemented a Q-Learning agent, applying the formula $Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha(R_{t+1} + \\gamma max_a Q(S_{t+1}, a) - Q(S_t, A_t))$ to update the q-table. The q-table is updated every action (instead of every episode like in Montecarlo), which means that we can take advantage of that to reward specific behaviours such as blocking an opponent's win.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_action(state: BoardState, action: int, player: int) -> BoardState:\n",
    "    new_state = deepcopy(state)\n",
    "    new_state.play(action, player)\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(current_state: BoardState, next_state: BoardState, action: int) -> float:\n",
    "    available = current_state.get_available()\n",
    "    if action not in available:\n",
    "        return -100\n",
    "    elif is_block(current_state, action):\n",
    "        return 5\n",
    "    elif next_state.x_win():\n",
    "        return 10\n",
    "    elif next_state.o_win():\n",
    "        return -10\n",
    "    return 0\n",
    "\n",
    "\n",
    "def is_block(state: BoardState, action: int) -> float:\n",
    "    opponent_tiles = state.o_plays\n",
    "    return sum(opponent_tiles) + action == GOAL_SUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f1dcaad89649c192b70ab5a6dbef9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q_table = defaultdict(lambda: np.random.uniform(low=-0.1, high=0.1))\n",
    "\n",
    "\n",
    "DISCOUNT_RATE = 0.99\n",
    "LEARNING_RATE = 0.01\n",
    "EPISODES = 1_000_000\n",
    "EPSILON_DECAY = 0.995\n",
    "epsilon = 0.25\n",
    "start_state = BoardState()\n",
    "episode_rewards = []\n",
    "\n",
    "\n",
    "for episode in tqdm(range(EPISODES)):\n",
    "    current_state = start_state\n",
    "    total_reward = 0\n",
    "\n",
    "    while not (current_state.x_win() or current_state.o_win()) and len(\n",
    "        current_state.get_available()\n",
    "    ):\n",
    "        if random() < episode:  # explore\n",
    "            new_action = randint(0, BOARD_SIZE**2)\n",
    "        else:  # exploit\n",
    "            hashable_state = current_state.get_hashable()\n",
    "            possible_actions = [x for x in range(BOARD_SIZE**2)]\n",
    "            q_values = [q_table[(hashable_state, a)] for a in possible_actions]\n",
    "            new_action = possible_actions[np.argmax(q_values)]\n",
    "\n",
    "        new_state = do_action(current_state, new_action, X)\n",
    "        new_reward = reward(current_state, new_state, new_action)\n",
    "        hashable_state_action = (current_state.get_hashable(), new_action)\n",
    "\n",
    "        current_q = q_table[hashable_state_action]\n",
    "\n",
    "        possible_states_actions = [\n",
    "            (do_action(new_state, a, O).get_hashable(), a)\n",
    "            for a in new_state.get_available()\n",
    "        ]\n",
    "\n",
    "        next_actions_and_states = [\n",
    "            q_table[state_action] for state_action in possible_states_actions\n",
    "        ]\n",
    "\n",
    "        if next_actions_and_states:\n",
    "            max_next_q = -np.max(next_actions_and_states)\n",
    "        else:\n",
    "            max_next_q = 0\n",
    "\n",
    "        target_q = new_reward + DISCOUNT_RATE * max_next_q\n",
    "        q_table[hashable_state_action] += LEARNING_RATE * (target_q - current_q)\n",
    "\n",
    "        total_reward += new_reward\n",
    "\n",
    "        if new_state.get_available():\n",
    "            new_state = do_action(new_state, choice(list(new_state.get_available())), O)\n",
    "        current_state = new_state\n",
    "\n",
    "    epsilon *= EPSILON_DECAY\n",
    "    episode_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_move(state: BoardState) -> int:\n",
    "    possible = list(range(BOARD_SIZE**2))\n",
    "    best_q = float(\"-inf\")\n",
    "    best_move = None\n",
    "\n",
    "    for move in possible:\n",
    "        move_q = q_table[(state.get_hashable(), move)]\n",
    "        if move_q > best_q:\n",
    "            best_q = move_q\n",
    "            best_move = move\n",
    "\n",
    "    return best_move\n",
    "\n",
    "\n",
    "def random_move(state: BoardState) -> int:\n",
    "    possible = list(state.get_available())\n",
    "    return choice(possible)\n",
    "\n",
    "\n",
    "def game(selector_x: callable, selector_o: callable) -> BoardState:\n",
    "    current_state = BoardState()\n",
    "    current_player = X\n",
    "\n",
    "    while not (current_state.is_over()):\n",
    "        if current_player == X:\n",
    "            action = selector_x(current_state)\n",
    "        else:\n",
    "            action = selector_o(current_state)\n",
    "\n",
    "        temp_state = deepcopy(current_state)\n",
    "        temp_state.play(action, current_player)\n",
    "        current_state = temp_state\n",
    "\n",
    "        current_player = 1 - current_player\n",
    "\n",
    "    return current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-LEARNING AGAINST RANDOM Wins: 811, Draws: 158, Losses: 31\n",
      "Winning Percentage: 81.10%\n",
      "Draw Percentage: 15.80%\n",
      "Loss Percentage: 3.10%\n"
     ]
    }
   ],
   "source": [
    "SIMULATIONS = 1_000\n",
    "\n",
    "\n",
    "def simulate_games(n_games: int, move_x: callable, move_y: callable) -> tuple[int]:\n",
    "    x_wins = 0\n",
    "    o_wins = 0\n",
    "    draws = 0\n",
    "    for _ in range(n_games):\n",
    "        game_state = game(move_x, move_y)\n",
    "        if game_state.x_win():\n",
    "            x_wins += 1\n",
    "        elif game_state.o_win():\n",
    "            o_wins += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "    return x_wins, draws, o_wins\n",
    "\n",
    "\n",
    "q_wins, draws, opponent_wins = simulate_games(SIMULATIONS, q_move, random_move)\n",
    "\n",
    "total_games = q_wins + opponent_wins + draws\n",
    "winning_percentage = q_wins / total_games if total_games != 0 else 0\n",
    "draw_percentage = draws / total_games if total_games != 0 else 0\n",
    "loss_percentage = opponent_wins / total_games if total_games != 0 else 0\n",
    "\n",
    "print(\n",
    "    f\"Q-LEARNING AGAINST RANDOM Wins: {q_wins}, Draws: {draws}, Losses: {opponent_wins}\"\n",
    ")\n",
    "print(\n",
    "    f\"Winning Percentage: {winning_percentage:.2%}\\nDraw Percentage: {draw_percentage:.2%}\\nLoss Percentage: {loss_percentage:.2%}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
